---
created: 2024-01-22 08:44
aliases: 
tags:
  - ChangeLog
  - Scheduler
card link:
  - "[[Re-Implement Bitcask Database]]"
source:
---
## Description
---

The current design of the dump file mechanism is implemented in with a task pool, scheduler, workers and snapshot mechanism.

### Requirements
- We need a mechanism to dump memory into file asynchronously, since, we don’t want our database to have any significant downtime.
- We need to make sure even the memory is in converting or waiting queue, we are still able to read the data inside the memory.
- For BitCask /LSM tree storage design purpose, while reading the memory, the order of reading from task queue is important. Since there might be duplicate keys, the latest memory should be read first.

### Design Flow

![[AsychronousScheduler.png]]

### Design Pros and Cons
#### Pros
- The design is ok with asynchronous version. It works and there is not much downtime while processing the “dump memory to file operation”.
	- Note: The approach to “High Availability”.
- The data lies in the memory is snapshot in the first place before entering the task queue. Which makes it easy to manage.

#### Cons
- Since each time the memory reaches the limits, it does a snapshot. Therefore, it doubles the memory usage in certain point.
- As all unprocessed memory were stored in the task pool until it has been processed. The software memory increases each time the new snapshot memory were added to the tasks. Until the worker finishes the dump process and remove it from the task pool. (This seems unavoidable)
- Cloning singleton memory in the first place takes at least O(n) to process.
- Reset the singleton memory in the first place and delete the memory from task pool causes a lot of GC pressure.
- The data were distributed in three places. (Too many places)
	- In memory, singleton memory
	- In memory, task pool snapshots
	- In segment, disks.

#### Main Problem:
1. Doing snapshot in the first place doubles the memory will cause a peak in some moments.
2. Reset the singleton memory generates a lot of GC pressure. 
3. After the worker finishes writing to file, deletes the snapshot in task pool generates a lot of GC pressure. Especially, there are multiple goroutine finished the work in the same time.
4. The data is distributed across to many places, which should limited somehow to 
	- In memory, only one places
	- In segment, disks.
### Reference
---





